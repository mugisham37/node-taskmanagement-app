name: Database Backup & Maintenance

on:
  schedule:
    # Daily backup at 2 AM UTC
    - cron: '0 2 * * *'
    # Weekly full backup on Sundays at 1 AM UTC
    - cron: '0 1 * * 0'
  workflow_dispatch:
    inputs:
      backup_type:
        description: 'Type of backup to perform'
        required: true
        default: 'incremental'
        type: choice
        options:
          - incremental
          - full
          - schema-only
      environment:
        description: 'Environment to backup'
        required: true
        default: 'production'
        type: choice
        options:
          - production
          - staging

env:
  AWS_REGION: us-east-1
  BACKUP_BUCKET: taskmanagement-db-backups

jobs:
  # Database backup
  backup:
    name: Database Backup
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'production' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client-15

      - name: Determine backup type
        id: backup-type
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "type=${{ github.event.inputs.backup_type }}" >> $GITHUB_OUTPUT
            echo "env=${{ github.event.inputs.environment }}" >> $GITHUB_OUTPUT
          elif [ "$(date +%u)" = "7" ]; then
            # Sunday - full backup
            echo "type=full" >> $GITHUB_OUTPUT
            echo "env=production" >> $GITHUB_OUTPUT
          else
            # Weekday - incremental backup
            echo "type=incremental" >> $GITHUB_OUTPUT
            echo "env=production" >> $GITHUB_OUTPUT
          fi

      - name: Create backup directory
        run: |
          mkdir -p backups
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          echo "TIMESTAMP=$TIMESTAMP" >> $GITHUB_ENV
          echo "BACKUP_DIR=backups/${{ steps.backup-type.outputs.env }}_${{ steps.backup-type.outputs.type }}_$TIMESTAMP" >> $GITHUB_ENV

      - name: Perform database backup
        run: |
          mkdir -p $BACKUP_DIR
          
          case "${{ steps.backup-type.outputs.type }}" in
            "full")
              echo "Performing full database backup..."
              pg_dump $DATABASE_URL \
                --verbose \
                --format=custom \
                --compress=9 \
                --file=$BACKUP_DIR/full_backup.dump
              
              # Also create a plain SQL backup for easier restoration
              pg_dump $DATABASE_URL \
                --verbose \
                --format=plain \
                --file=$BACKUP_DIR/full_backup.sql
              ;;
            
            "incremental")
              echo "Performing incremental backup..."
              # Get the last backup timestamp
              LAST_BACKUP=$(aws s3 ls s3://$BACKUP_BUCKET/${{ steps.backup-type.outputs.env }}/ --recursive | grep incremental | tail -1 | awk '{print $1" "$2}')
              
              if [ -n "$LAST_BACKUP" ]; then
                LAST_BACKUP_DATE=$(date -d "$LAST_BACKUP" +%Y-%m-%d\ %H:%M:%S)
                echo "Last backup: $LAST_BACKUP_DATE"
                
                # Export data modified since last backup
                psql $DATABASE_URL -c "COPY (
                  SELECT 'tasks' as table_name, row_to_json(t) as data 
                  FROM tasks t 
                  WHERE updated_at > '$LAST_BACKUP_DATE'
                  UNION ALL
                  SELECT 'projects' as table_name, row_to_json(p) as data 
                  FROM projects p 
                  WHERE updated_at > '$LAST_BACKUP_DATE'
                  UNION ALL
                  SELECT 'users' as table_name, row_to_json(u) as data 
                  FROM users u 
                  WHERE updated_at > '$LAST_BACKUP_DATE'
                ) TO STDOUT WITH CSV HEADER" > $BACKUP_DIR/incremental_backup.csv
              else
                echo "No previous backup found, performing full backup instead"
                pg_dump $DATABASE_URL \
                  --verbose \
                  --format=custom \
                  --compress=9 \
                  --file=$BACKUP_DIR/full_backup.dump
              fi
              ;;
            
            "schema-only")
              echo "Performing schema-only backup..."
              pg_dump $DATABASE_URL \
                --verbose \
                --schema-only \
                --format=plain \
                --file=$BACKUP_DIR/schema_backup.sql
              ;;
          esac
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}

      - name: Compress backup files
        run: |
          cd backups
          tar -czf ${{ steps.backup-type.outputs.env }}_${{ steps.backup-type.outputs.type }}_$TIMESTAMP.tar.gz \
            ${{ steps.backup-type.outputs.env }}_${{ steps.backup-type.outputs.type }}_$TIMESTAMP/
          
          # Calculate checksum
          sha256sum ${{ steps.backup-type.outputs.env }}_${{ steps.backup-type.outputs.type }}_$TIMESTAMP.tar.gz > \
            ${{ steps.backup-type.outputs.env }}_${{ steps.backup-type.outputs.type }}_$TIMESTAMP.tar.gz.sha256

      - name: Upload backup to S3
        run: |
          cd backups
          
          # Upload compressed backup
          aws s3 cp ${{ steps.backup-type.outputs.env }}_${{ steps.backup-type.outputs.type }}_$TIMESTAMP.tar.gz \
            s3://$BACKUP_BUCKET/${{ steps.backup-type.outputs.env }}/${{ steps.backup-type.outputs.type }}/
          
          # Upload checksum
          aws s3 cp ${{ steps.backup-type.outputs.env }}_${{ steps.backup-type.outputs.type }}_$TIMESTAMP.tar.gz.sha256 \
            s3://$BACKUP_BUCKET/${{ steps.backup-type.outputs.env }}/${{ steps.backup-type.outputs.type }}/
          
          # Update latest backup pointer
          echo "${{ steps.backup-type.outputs.env }}_${{ steps.backup-type.outputs.type }}_$TIMESTAMP.tar.gz" | \
            aws s3 cp - s3://$BACKUP_BUCKET/${{ steps.backup-type.outputs.env }}/latest_${{ steps.backup-type.outputs.type }}.txt

      - name: Verify backup integrity
        run: |
          cd backups
          
          # Download and verify checksum
          aws s3 cp s3://$BACKUP_BUCKET/${{ steps.backup-type.outputs.env }}/${{ steps.backup-type.outputs.type }}/${{ steps.backup-type.outputs.env }}_${{ steps.backup-type.outputs.type }}_$TIMESTAMP.tar.gz.sha256 \
            downloaded_checksum.sha256
          
          # Verify checksum
          sha256sum -c downloaded_checksum.sha256
          
          echo "Backup integrity verified successfully!"

      - name: Update backup metadata
        run: |
          # Create backup metadata
          cat > backup_metadata.json << EOF
          {
            "timestamp": "$TIMESTAMP",
            "type": "${{ steps.backup-type.outputs.type }}",
            "environment": "${{ steps.backup-type.outputs.env }}",
            "size_bytes": $(stat -c%s backups/${{ steps.backup-type.outputs.env }}_${{ steps.backup-type.outputs.type }}_$TIMESTAMP.tar.gz),
            "checksum": "$(cat backups/${{ steps.backup-type.outputs.env }}_${{ steps.backup-type.outputs.type }}_$TIMESTAMP.tar.gz.sha256 | cut -d' ' -f1)",
            "database_version": "$(psql $DATABASE_URL -t -c 'SELECT version();' | xargs)",
            "backup_tool": "pg_dump",
            "compression": "gzip"
          }
          EOF
          
          # Upload metadata
          aws s3 cp backup_metadata.json \
            s3://$BACKUP_BUCKET/${{ steps.backup-type.outputs.env }}/metadata/${{ steps.backup-type.outputs.env }}_${{ steps.backup-type.outputs.type }}_$TIMESTAMP.json
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}

  # Database maintenance
  maintenance:
    name: Database Maintenance
    runs-on: ubuntu-latest
    needs: backup
    if: github.event.schedule == '0 1 * * 0' # Only run on weekly schedule
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client-15

      - name: Run database maintenance
        run: |
          echo "Starting database maintenance..."
          
          # Analyze database statistics
          psql $DATABASE_URL -c "ANALYZE;"
          
          # Vacuum database (reclaim storage)
          psql $DATABASE_URL -c "VACUUM (ANALYZE, VERBOSE);"
          
          # Reindex database
          psql $DATABASE_URL -c "REINDEX DATABASE taskmanagement;"
          
          # Update table statistics
          psql $DATABASE_URL -c "
            SELECT schemaname, tablename, attname, n_distinct, correlation 
            FROM pg_stats 
            WHERE schemaname = 'public' 
            ORDER BY tablename, attname;
          " > database_stats.txt
          
          echo "Database maintenance completed successfully!"
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}

      - name: Check database health
        run: |
          echo "Checking database health..."
          
          # Check database size
          psql $DATABASE_URL -c "
            SELECT 
              pg_database.datname,
              pg_size_pretty(pg_database_size(pg_database.datname)) AS size
            FROM pg_database
            WHERE datname = 'taskmanagement';
          "
          
          # Check table sizes
          psql $DATABASE_URL -c "
            SELECT 
              schemaname,
              tablename,
              pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,
              pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) AS table_size,
              pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) AS index_size
            FROM pg_tables
            WHERE schemaname = 'public'
            ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
          "
          
          # Check for long-running queries
          psql $DATABASE_URL -c "
            SELECT 
              pid,
              now() - pg_stat_activity.query_start AS duration,
              query 
            FROM pg_stat_activity 
            WHERE (now() - pg_stat_activity.query_start) > interval '5 minutes'
            AND state = 'active';
          "
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}

      - name: Upload maintenance report
        uses: actions/upload-artifact@v3
        with:
          name: database-maintenance-report
          path: database_stats.txt

  # Backup cleanup
  cleanup:
    name: Cleanup Old Backups
    runs-on: ubuntu-latest
    needs: backup
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Cleanup old backups
        run: |
          echo "Cleaning up old backups..."
          
          # Keep last 30 daily backups
          aws s3 ls s3://$BACKUP_BUCKET/production/incremental/ | \
            sort -k1,2 | \
            head -n -30 | \
            awk '{print $4}' | \
            while read file; do
              if [ -n "$file" ]; then
                echo "Deleting old incremental backup: $file"
                aws s3 rm s3://$BACKUP_BUCKET/production/incremental/$file
                aws s3 rm s3://$BACKUP_BUCKET/production/incremental/${file}.sha256 2>/dev/null || true
                aws s3 rm s3://$BACKUP_BUCKET/production/metadata/${file%.*}.json 2>/dev/null || true
              fi
            done
          
          # Keep last 12 weekly full backups
          aws s3 ls s3://$BACKUP_BUCKET/production/full/ | \
            sort -k1,2 | \
            head -n -12 | \
            awk '{print $4}' | \
            while read file; do
              if [ -n "$file" ]; then
                echo "Deleting old full backup: $file"
                aws s3 rm s3://$BACKUP_BUCKET/production/full/$file
                aws s3 rm s3://$BACKUP_BUCKET/production/full/${file}.sha256 2>/dev/null || true
                aws s3 rm s3://$BACKUP_BUCKET/production/metadata/${file%.*}.json 2>/dev/null || true
              fi
            done
          
          echo "Backup cleanup completed!"

  # Backup verification
  verify:
    name: Verify Backup Integrity
    runs-on: ubuntu-latest
    needs: backup
    if: github.event.schedule == '0 1 * * 0' # Only run weekly verification
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client-15

      - name: Download and verify latest backup
        run: |
          # Get latest full backup
          LATEST_BACKUP=$(aws s3 cp s3://$BACKUP_BUCKET/production/latest_full.txt -)
          
          echo "Verifying backup: $LATEST_BACKUP"
          
          # Download backup and checksum
          aws s3 cp s3://$BACKUP_BUCKET/production/full/$LATEST_BACKUP backup.tar.gz
          aws s3 cp s3://$BACKUP_BUCKET/production/full/${LATEST_BACKUP}.sha256 backup.tar.gz.sha256
          
          # Verify checksum
          sha256sum -c backup.tar.gz.sha256
          
          # Extract backup
          tar -xzf backup.tar.gz
          
          echo "Backup integrity verified successfully!"

      - name: Test backup restoration (dry run)
        run: |
          # Start a temporary PostgreSQL instance for testing
          docker run -d --name postgres-test \
            -e POSTGRES_PASSWORD=test \
            -e POSTGRES_DB=test_restore \
            -p 5433:5432 \
            postgres:15
          
          # Wait for PostgreSQL to start
          sleep 30
          
          # Test restoration
          BACKUP_FILE=$(find . -name "*.dump" | head -1)
          if [ -n "$BACKUP_FILE" ]; then
            echo "Testing restoration of $BACKUP_FILE"
            pg_restore -h localhost -p 5433 -U postgres -d test_restore -v "$BACKUP_FILE" || true
            
            # Verify some basic data
            psql -h localhost -p 5433 -U postgres -d test_restore -c "
              SELECT 
                schemaname, 
                tablename, 
                COUNT(*) as row_count 
              FROM information_schema.tables t
              LEFT JOIN (
                SELECT 
                  schemaname, 
                  tablename, 
                  n_tup_ins as row_count
                FROM pg_stat_user_tables
              ) s USING (schemaname, tablename)
              WHERE t.table_schema = 'public'
              GROUP BY schemaname, tablename;
            "
          fi
          
          # Cleanup
          docker stop postgres-test
          docker rm postgres-test
        env:
          PGPASSWORD: test

  # Notification
  notify-backup:
    name: Notify Backup Status
    runs-on: ubuntu-latest
    needs: [backup, maintenance, cleanup, verify]
    if: always()
    steps:
      - name: Notify on success
        if: needs.backup.result == 'success'
        uses: 8398a7/action-slack@v3
        with:
          status: success
          channel: '#infrastructure'
          text: |
            ✅ Database backup completed successfully!
            
            Backup Details:
            - Type: ${{ needs.backup.outputs.type || 'scheduled' }}
            - Environment: ${{ needs.backup.outputs.env || 'production' }}
            - Timestamp: $(date)
            
            Additional Tasks:
            - Maintenance: ${{ needs.maintenance.result || 'skipped' }}
            - Cleanup: ${{ needs.cleanup.result }}
            - Verification: ${{ needs.verify.result || 'skipped' }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

      - name: Notify on failure
        if: failure()
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          channel: '#infrastructure'
          text: |
            🚨 Database backup failed!
            
            Failed Tasks:
            - Backup: ${{ needs.backup.result }}
            - Maintenance: ${{ needs.maintenance.result }}
            - Cleanup: ${{ needs.cleanup.result }}
            - Verification: ${{ needs.verify.result }}
            
            Please check the workflow logs and ensure database backups are working properly.
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}